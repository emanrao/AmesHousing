{
  "cells": [
    {
      "metadata": {
        "_uuid": "3c44d0f673d7d5bd77f1db8911b425ad556c2f77"
      },
      "cell_type": "markdown",
      "source": "# Load libraries and files"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import Libraries\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.stats.stats import pearsonr\n\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer, r2_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom IPython.display import display\npd.set_option('display.float_format', lambda x: '%.3f' %x)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Look for Files\nimport os\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make dataframe for train and test sets\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\nprint('Train dataframe size: ', df_train.shape)\nprint('Test dataframe size: ', df_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "045b340044aaf7de3b9b210bf24d63f197cc4261"
      },
      "cell_type": "code",
      "source": "# View sample of data\ndf_train.sample(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "933bd74f65688e7f95b5938b493e27d4b341c13f"
      },
      "cell_type": "code",
      "source": "# Get list of all features\nfeatures = df_train.columns.tolist()\nprint('Features:')\nfeatures",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90c8922806b461c4ae97205a3f724e164dfc1d26"
      },
      "cell_type": "code",
      "source": "# Describe features\ndf_train.describe(include='all')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d3baa17ec92ae28bedab00f415f2e20bfa77dc6"
      },
      "cell_type": "code",
      "source": "# Make sure there are no duplicate ids\nidsUnique = len(set(df_train.Id))\nidsTotal = df_train.shape[0]\nidsDup = idsTotal - idsUnique\nif idsDup>0: print('DUPLICATE IDS FOUND')\n    \n# Remove ID column from dataframe\ntrain_IDs = df_train.Id\ndf_train.drop('Id', axis=1, inplace=True)\ntest_IDs = df_test.Id\ndf_test.drop('Id', axis=1, inplace=True)\nfeatures.remove('Id')\n\nprint('ID column dropped')\nprint('Train dataframe size: ', df_train.shape)\nprint('Test dataframe size: ', df_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96866d2f5830ff6b8ceea0f155e676afa73e0dc5"
      },
      "cell_type": "markdown",
      "source": "# Coorelate features to eachother and to sales price"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e7a7976aa64bd0711a18d998de2f1925ad96449"
      },
      "cell_type": "code",
      "source": "# Coorelation plot\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(20,9))\nsns.heatmap(corrmat, square=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72b4b36f7b2a36b6dcebfb0092c4d5647cd613fd"
      },
      "cell_type": "code",
      "source": "# Identify which features are most coorelated to eachother (redundant features)\ncorr_cutoff = 0.7\ntop_corrmat = pd.DataFrame(columns=['Feature_Correlation','Feature_1', 'Price_Correlation_1','Feature_2', 'Price_Correlation_2'])\nfor feature in corrmat.columns.tolist():\n    temp_list = corrmat.index[abs(corrmat[feature])>corr_cutoff].tolist()\n    temp_list.remove(feature) #remove diagnol elements\n    if 'SalePrice' in temp_list: temp_list.remove('SalePrice') #ignore sale price for now\n    if (len(temp_list)>0) & (feature != 'SalePrice'):\n        for each in temp_list:\n            if each not in top_corrmat['Feature_1'].values.tolist():\n                feature_corr = corrmat.loc[feature, each]\n                f1_sale = corrmat.loc[feature, 'SalePrice']\n                f2_sale = corrmat.loc[each, 'SalePrice']\n                top_corrmat = top_corrmat.append({'Feature_1': feature, 'Price_Correlation_1': f1_sale,'Feature_2': each, 'Feature_Correlation': feature_corr, 'Price_Correlation_2':f2_sale}, ignore_index=True)\n\nprint('Top Coorelated Features (cutoff = 0.7):')\ntop_corrmat",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72082bdd7f81cb0c6d2bc86c13fc33e26ec4e43a"
      },
      "cell_type": "code",
      "source": "# For each pair remove feature less coorelated with price\nfeatures.remove('GarageYrBlt')\nfeatures.remove('1stFlrSF')\nfeatures.remove('TotRmsAbvGrd')\nfeatures.remove('GarageArea')\nprint('Updated Features List:')\nfeatures",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e39f30b9d16f4986345102f503405ad00304e12b"
      },
      "cell_type": "code",
      "source": "# Update datagframes with new feature list\ndf_train = df_train[features]\ntemp_features = [i for i in features if i != 'SalePrice']\ndf_test = df_test[temp_features]\n\n# Correlation plot\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(20,9))\nsns.heatmap(corrmat, square=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "703236ae86ff4ce3e94e877228792bb1d04a3075"
      },
      "cell_type": "code",
      "source": "# Find features most coorelated with sale price\ncorr_cutoff = 0.5\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>corr_cutoff]\nplt.figure(figsize=(10,10))\ng=sns.heatmap(df_train[top_corr_features].corr(), annot=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c6772f971f168a7609548e9dd4ccac49794e94"
      },
      "cell_type": "code",
      "source": "# Plot for most coorelated feature\nsns.boxplot(x='OverallQual', y='SalePrice', data=df_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9b58590433952f814953d6d49440cc7cc26db547"
      },
      "cell_type": "code",
      "source": "# Scatterplots for top features\ntop_corr_features.drop('SalePrice')\nplt.figure(figsize=(20,20))\nnrows = str(int(np.ceil(len(top_corr_features)/3)))\nncols = str(3)\ni=1\nfor feature in top_corr_features:\n    plt.subplot(nrows, ncols, i)\n    plt.scatter(df_train[feature], df_train.SalePrice)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    i += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0c156144b469a4cdc7b9cce2a786d94256e4a87"
      },
      "cell_type": "code",
      "source": "# Remove outliers\ndf_train = df_train[df_train.TotalBsmtSF < 5000]\ndf_train = df_train[(df_train.GrLivArea < 4000) | (df_train.SalePrice > 600000)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "924adce4f24d313a2e21c6bcb9723398d44cc4eb"
      },
      "cell_type": "code",
      "source": "# Replot distributions\nplt.figure(figsize=(20,20))\nnrows = str(int(np.ceil(len(top_corr_features)/3)))\nncols = str(3)\ni=1\nfor feature in top_corr_features:\n    plt.subplot(nrows, ncols, i)\n    plt.scatter(df_train[feature], df_train.SalePrice)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    i += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05e314a28282686554721ee1ee4b94a1138da4cc"
      },
      "cell_type": "markdown",
      "source": "# Create combined feature dataframe for data cleaning\n# Divide up numerical and categorical features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8555bd5f1b0e1b10d59a2b3fa9e3f8e70b5bed6"
      },
      "cell_type": "code",
      "source": "# Save number of entries for each dataframe\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\n# combine train and test data with train on top\nall_data = pd.concat((df_train, df_test))\n\n# drop SalePrice from features\nall_data.drop('SalePrice', axis=1, inplace=True)\n\nprint ('Dataframe sizes:')\nprint('Train: ', df_train.shape)\nprint('Test: ', df_test.shape)\nprint('All Data', all_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1203d044cb042e0e6d12b8b8572a6471927313c"
      },
      "cell_type": "code",
      "source": "# Separate feeatures into numerical and calegorical\ncat_features = all_data.select_dtypes(include=['object']).columns.values.tolist()\nnum_features = all_data.select_dtypes(exclude=['object']).columns.values.tolist()\nprint(len(cat_features), ' Cat Features')\nprint(cat_features)\nprint(len(num_features), ' Num Features')\nprint(num_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eecd42b25bde4e36ddcbd52e34b64ab35dcc76a"
      },
      "cell_type": "code",
      "source": "# recategorize cat features that look like num features\nnew_cat = ['MSSubClass', 'MoSold', 'OverallCond', 'OverallQual']\nfor each in new_cat:\n    cat_features.append(each)\n    num_features.remove(each)\nprint(len(cat_features), ' Cat Features')\nprint(cat_features)\nprint(len(num_features), ' Num Features')\nprint(num_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6827a46406e117b62798fa560a4a701e88aa1476"
      },
      "cell_type": "code",
      "source": "# Save dataframes for each feature type\nnum_data = all_data[num_features]\ncat_data = all_data[cat_features]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c79c8da7514b03253ef0118a7d6b166695430a96"
      },
      "cell_type": "markdown",
      "source": "# Find Missing Values\n\n## Fill with specific value\n### NA\nAlley            object\n\nFence            object \n\nFireplaceQu      object\n\nMiscFeature      object \n\nPoolQC           object \n\n\n### Typ\nFunctional       object\n\n### TA\nKitchenQual      object\n\n### Median of Neighborhood\nLotFrontage     float64\n\n### Mode of Neighborhood\nMSZoning         object \n\nSaleType         object \n\nUtilities        object \n\nElectrical       object \n\nExterior1st      object \n\nExterior2nd      object \n\n\n### 0\nMasVnrArea      float64 \n\n### None\nMasVnrType       object \n\n## Determine if area exists\nlook for cat feature other than NA or sum of num features  >0\nif exists, fill with mode/median of neighborhood\nelse fill with None/0\n\n### Basement\nBsmtCond         object \n\nBsmtExposure     object \n\nBsmtFinType1     object\n\nBsmtFinType2     object \n\nBsmtQual         object \n\n\nBsmtFinSF1      float64 \n\nBsmtFinSF2      float64 \n\nBsmtFullBath    float64 \n\nBsmtHalfBath    float64 \n\nBsmtUnfSF       float64 \n\nTotalBsmtSF     float64 \n\n\n### Garage\nGarageCond       object \n\nGarageFinish     object \n\nGarageQual       object \n\nGarageType       object \n\n\nGarageArea      float64 \n\nGarageCars      float64 \n\nGarageYrBlt     float64 \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65fe94344271b8034543dfbe15f2ad6623eeb110"
      },
      "cell_type": "code",
      "source": "# Find which features have missing data\nall_data_missing = all_data.isnull().sum()\nall_data_missing = all_data_missing[all_data_missing>0]\nall_data_missing.sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f95cce61d3d9df9e63dfd4e4688a335e31007f51"
      },
      "cell_type": "code",
      "source": "all_data[all_data_missing.index].dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4db565af53b15d610545a68510856f45695ad5f"
      },
      "cell_type": "code",
      "source": "# Look for coorelated features to detemine importance of each feature\ncorr_cutoff = 0.5\nfor each in all_data_missing.index:\n    if all_data[each].dtype =='float64':\n        top_features = corrmat.index[abs(corrmat[each])>corr_cutoff].values.tolist()\n        top_features.remove(each)\n        for ea in top_features:\n            if ea not in all_data_missing.index:\n                print(each, ': ', ea, ', ', corrmat[each][ea])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31465c186dac04ec31ea9100a78b5c3c314deeb8"
      },
      "cell_type": "code",
      "source": "# Fill missing values as listed in above comment\ntemp_list=['Alley', 'Fence', 'FireplaceQu', 'MiscFeature', 'PoolQC']\ntemp_value='NA'\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.loc[:, each].fillna(temp_value)\n\ntemp_list=['Functional']\ntemp_value='Typ'\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.loc[:, each].fillna(temp_value)\n\ntemp_list=['KitchenQual']\ntemp_value='TA'\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.loc[:, each].fillna(temp_value)\n\ntemp_list=['MasVnrType']\ntemp_value='None'\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.loc[:, each].fillna(temp_value)\n    \ntemp_list=['MasVnrArea']\ntemp_value=0\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.loc[:, each].fillna(temp_value)\n\ntemp_list=['LotFrontage']\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.median()))\n\ntemp_list=['MSZoning', 'SaleType', 'Utilities', 'Electrical', 'Exterior1st', 'Exterior2nd']\nfor each in temp_list:\n    all_data.loc[:, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.mode()[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74f631890e6944534e9c6e912d147237a1ba3cc8"
      },
      "cell_type": "code",
      "source": "# Basement Features\ncat_Bsmt = [i for i in cat_features if 'Bsmt' in i]\nnum_Bsmt = [i for i in num_features if 'Bsmt' in i]\nall_Bsmt = cat_Bsmt + num_Bsmt\ntemp_array = []\nfor index, row in all_data[all_data[all_Bsmt].isnull().any(axis=1)].iterrows():\n    include=False\n    for each in num_Bsmt:\n        if row[each]>0: include=True\n    for each in cat_Bsmt:\n        if type(row[each])==str:\n            if row[each] != 'NA': include=True\n    if include: \n        temp_array.append(index)\n        for each in num_Bsmt:\n            all_data.loc[index, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.median())).iloc[index]\n        for each in cat_Bsmt:\n            all_data.loc[index, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.mode()[0])).iloc[index]\n    else:\n        for each in num_Bsmt:\n            all_data.loc[index, each] = 0\n        for each in cat_Bsmt:\n            all_data.loc[index, each] = 'NA'\n\nprint('Remaining missing data for Basement:')\nall_data[all_Bsmt].isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "279053d5253909bed6aed6ff12966aeeebc1ae37"
      },
      "cell_type": "code",
      "source": "# Garage Features\ncat_Gar = [i for i in cat_features if 'Garage' in i]\nnum_Gar = [i for i in num_features if 'Garage' in i]\nall_Gar = cat_Gar + num_Gar\ntemp_array = []\nfor index, row in all_data[all_data[all_Gar].isnull().any(axis=1)].iterrows():\n    include=False\n    for each in num_Gar:\n        if row[each]>0: include=True\n    for each in cat_Gar:\n        if type(row[each])==str:\n            if row[each] != 'NA': include=True\n    if include: \n        temp_array.append(index)\n        for each in num_Gar:\n            all_data.loc[index, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.median())).iloc[index]\n        for each in cat_Gar:\n            all_data.loc[index, each] = all_data.groupby(by='Neighborhood')[each].transform(lambda x: x.fillna(x.mode()[0])).iloc[index]\n    else:\n        for each in num_Gar:\n            all_data.loc[index, each] = 0\n        for each in cat_Gar:\n            all_data.loc[index, each] = 'NA'\n\nprint('Remaining missing data for Garage:')\nall_data[all_Gar].isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f64f914529daabd1f14df34df52db72772881f0"
      },
      "cell_type": "code",
      "source": "num_data = all_data[num_features]\ncat_data = all_data[cat_features]\nprint('Number of Missing Values')\nnp.max(all_data.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e75e864bbcc346458b57becc7b1ae4b46d640b6"
      },
      "cell_type": "markdown",
      "source": "# Look at Sales Price (Target Value)\nPlot histogram of sales prices and compare to normal distribution\nSince there is a poor fit, take the log and try again"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d067d6d374eef17424c61588a50dede2f1c33776"
      },
      "cell_type": "code",
      "source": "# Plot sale prices against Normal Distribution\ndf_target = df_train.SalePrice\nsns.distplot(df_target, fit=norm)\n(mu, sigma) = norm.fit(df_target)\nprint('mu = {:2f} and sigma = {:2f}'.format(mu,sigma))\nplt.legend(['Normal Districution'], loc='best')\nplt.ylabel('Frequency')\nplt.title('Sale Price Distribution')\nfig = plt.figure()\nres = stats.probplot(df_target, plot=plt)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce39c1c88c362f24a16328f3c9abb5f39133320f"
      },
      "cell_type": "code",
      "source": "# Log transform target\ndf_target = np.log1p(df_target) #log of price + 1\nsns.distplot(df_target, fit=norm)\n(mu, sigma) = norm.fit(df_target)\nprint('mu = {:2f} and sigma = {:2f}'.format(mu,sigma))\nplt.legend(['Normal Districution'], loc='best')\nplt.ylabel('Frequency')\nplt.title('Sale Price Distribution')\nfig = plt.figure()\nres = stats.probplot(df_target, plot=plt)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2d692e069462898bb657c8c6b287d7ed4994ca4"
      },
      "cell_type": "markdown",
      "source": "# Numerical Features\nCheck for skew\nTake log"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "e5ca40ee64a104757870c9a7a0a26421a5f9e41e"
      },
      "cell_type": "code",
      "source": "# Check for skew and transforms those with high skew\nskew_cutoff = 0.5\nskew_values = num_data.apply(lambda x: skew(x))\nskew_values = skew_values[abs(skew_values)>skew_cutoff]\nprint(skew_values.sort_values(ascending=False))\nskew_features = skew_values.index\nnum_data[skew_features] = np.log1p(num_data[skew_features])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a84b73c5b66a9a18737137c4ac6dbfb3e9eea8d"
      },
      "cell_type": "code",
      "source": "# Normalize values\nstdSc = StandardScaler()\nnum_data.loc[:,num_features] = stdSc.fit_transform(num_data.loc[:,num_features])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7142efd639513d5a0b47b3cafb3a100f9a34c24e"
      },
      "cell_type": "code",
      "source": "# Plot distribution of transformed and normalized numerical features\nf=pd.melt(num_data, value_vars = num_features)\ng = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False)\ng=g.map(sns.distplot, \"value\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bbc2b12babf5e0f57763a638e7a0f23123623f04"
      },
      "cell_type": "markdown",
      "source": "# Categorical Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62f1208c84aaa6f691a668357c9323cb0a46edbb"
      },
      "cell_type": "code",
      "source": "# Plot Cateogrical Features\nfor each in cat_features:\n    cat_data[each] = cat_data[each].astype('category')\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x,y=y)\n    x=plt.xticks(rotation=90)\nf=pd.melt(df_train, id_vars=['SalePrice'], value_vars = cat_features)\ng = sns.FacetGrid(f, col='variable', col_wrap=2, sharex=False, sharey=False, size=5)\ng=g.map(boxplot, \"value\", \"SalePrice\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9192eb8c394db7103f79c83308c1d32c1db1e4af"
      },
      "cell_type": "code",
      "source": "# Test for differences between feature categories (ANOVA)\nanova = pd.DataFrame()\nanova['Feature'] = cat_features\nvalues = []\nfor feature in cat_features:\n    prices = []\n    for cat in df_train[feature].unique():\n        s = df_train[df_train[feature]==cat]['SalePrice'].values\n        prices.append(s)\n    pval = stats.f_oneway(*prices)[1]\n    values.append(pval)\nanova['Values'] = values\nanova.sort_values('Values', inplace=True)\nanova['Disparity'] = np.log(1./anova['Values'].values)\n\n# Plot disparity by category\nplt.figure(figsize=(10,5))\nsns.barplot(data=anova, x='Feature', y='Disparity')\nx=plt.xticks(rotation=90)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f26e354bc5c1758406bbdbde06a5ea44ff511882"
      },
      "cell_type": "code",
      "source": "# Create Dummy Variables for cateories\ncat_data = pd.get_dummies(cat_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b6e2c99091326d2eb48165781d73d01c3b43540"
      },
      "cell_type": "code",
      "source": "# Data Frame Sizes\nprint('Data Frame Sizes after expanding categorical features:')\nprint('Num Data: ', num_data.shape)\nprint('Cat Data: ', cat_data.shape)\nall_data = pd.concat([num_data, cat_data], axis=1, ignore_index=False)\nprint('All Data: ', all_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b648304531ca31cb027f319bf400c493a339486"
      },
      "cell_type": "markdown",
      "source": "# Identify uncoorelated Features to remove\nReduce number of features to avoid overfitting"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01d92b3ef0ef11d0a419faa4a370dd13ace77678"
      },
      "cell_type": "code",
      "source": "# use spearman coorlation to find features most coorelated with saleprice\nspearman = pd.DataFrame()\nfeatures = num_features + cat_data.columns.tolist()\nnew_df = all_data[0:df_target.shape[0]]\nspearman['Feature'] = features\nspearman['Spearman'] = [new_df[f].corr(df_train['SalePrice'], 'spearman') for f in features]\nspearman = spearman.sort_values('Spearman')\nplt.figure(figsize=(6, 0.25*len(features)))\nsns.barplot(data=spearman, y='Feature', x='Spearman', orient='h')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b96361ac84ca685f2208f8850376edf061c8747c"
      },
      "cell_type": "code",
      "source": "# refine features to those most coorelated to sale price\n# cutoff value chosen by comparing model error (below) for different values\nspearman_cutoff = 0.1\nrefined_features = spearman[np.abs(spearman['Spearman'])>spearman_cutoff].Feature.tolist()\nref_df = all_data[refined_features]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5475f5f2462be40215726fcedd2b7bafe119846"
      },
      "cell_type": "code",
      "source": "# Final dataframes\ntrain = ref_df[0:ntrain]\ntest = ref_df[ntrain:]\ntarget = df_target\n\n# Print sizes of dataframes for modeling\nprint('Data Frame Sizes:')\nprint('Training: ', train.shape)\nprint('Target: ' , target.shape)\nprint('Testing: ' , test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c2390bec8f3fb9d21300f60326a3aec8da1d11ff"
      },
      "cell_type": "markdown",
      "source": "# Modeling\nlinear regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a5cedafedf9f2ac7360cf6c43f59649c49c65bc"
      },
      "cell_type": "code",
      "source": "# Divide training data for cross validation\ntest_size = 0.3\nX_train, X_val, y_train, y_val = train_test_split(train, target,test_size = test_size, random_state=0)\nprint('Data divided for cross validation')\nprint('X_train: ', X_train.shape[0])\nprint('X_val: ', X_val.shape[0])\nprint('y_train: ', y_train.shape[0])\nprint('y_val: ', y_val.shape[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ac85b4372c418ef3f21daaf9c1da2462bf442ed"
      },
      "cell_type": "code",
      "source": "# Define scoring with cross validation\nn_folds = 5\ndef rmse_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = n_folds))\n    return(rmse)\ndef rmse_val(model):\n    rmse= np.sqrt(-cross_val_score(model, X_val, y_val, scoring=\"neg_mean_squared_error\", cv = n_folds))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a73e19a95a64e869cb2379cd136e6192a0b36fd"
      },
      "cell_type": "code",
      "source": "#  Keep track of model validation errors\nmodels = {}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc7882c428d6fad2bfe0497c3acaa2156354eff4"
      },
      "cell_type": "code",
      "source": "# Linear Regression without Regularization\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint ('Linear Regression')\nprint('Training Error: ', rmse_train(lr).mean())\nprint('Validation Error: ', rmse_val(lr).mean())\nmodels['LinearRegression'] = rmse_val(lr).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b48592aed3d6bce0ef7e451b0a1f12bc7294470c"
      },
      "cell_type": "code",
      "source": "# Ridge Regression\nprint('Ridge CV')\n\n# Find alpha\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\n# Eliminated features\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\n\n# Get model error\nprint('Training Error: ', rmse_train(ridge).mean())\nprint('Validation Error: ', rmse_val(ridge).mean())\nmodels['RidgeCV'] = rmse_val(ridge).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04089c5ebc63d84248d608a4ef26897a586a17f1"
      },
      "cell_type": "code",
      "source": "# Lasso Regression\nprint('Lasso CV')\n\n# Find alpha\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1,0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\n# Eliminated features\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\n\n# Get model error\nprint('Training Error: ', rmse_train(lasso).mean())\nprint('Validation Error: ', rmse_val(lasso).mean())\nmodels['LassoCV'] = rmse_val(lasso).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58a96d520e2c34de8a29b7ee4631bac5cbd39937"
      },
      "cell_type": "code",
      "source": "models",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba172c30dd1970fa5bbcd634cc6e1561c97306f6"
      },
      "cell_type": "markdown",
      "source": "# Run Model on Test Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9399b08887061981440b3ee177d0709a9e6b1b6a"
      },
      "cell_type": "code",
      "source": "test['SalePrice'] = np.expm1(lasso.predict(test))\ntest['Id'] = test_IDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2142ff3f6aed5b6328b294283e685b41fb54414"
      },
      "cell_type": "code",
      "source": "# Final submission file\ndf_submit = test[['Id', 'SalePrice']]\ndf_submit.to_csv(\"../working/ridgeCV.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66fe23f577eccea39fb7fcb0848bbe05380878bd"
      },
      "cell_type": "code",
      "source": "df_submit.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "900aab1803624352117d607da3d16fbab25fd2c5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}